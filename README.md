## Overview

The `Inference` package serves as a core foundation for building AI/LLM (Large Language Model) providers, enabling integration with various AI/LLM platforms like OpenAI, Anthropic, and offline models via Ollama. It abstracts away the complexities of interacting with multiple LLM services by offering a standardized interface for inference requests, handling responses, and managing interactions, making it easier to integrate and scale AI-driven capabilities in applications.
